#'
#' # The result from both queries is a data.frame.
rw.query <- function(entity = NULL,
limit = NULL,
text.query = NULL,
query.field = NULL,
query.field.value = NULL,
add.fields = NULL,
from = NULL,
to = NULL,
debug = FALSE,
csv = FALSE) {
#### Validation tests. ####
# The validation tests before are useful for helping the user make a
# right query and understand why his query isn't working.
# Install depencency packages if not installed
if ("lubridate" %in% rownames(installed.packages()) == FALSE) install.packages("lubridate")
if ("RCurl" %in% rownames(installed.packages()) == FALSE) install.packages("RCurl")
if ("jsonlite" %in% rownames(installed.packages()) == FALSE) install.packages("jsonlite")
if (is.null(query.field) == TRUE && is.null(text.query) == TRUE) { stop("You have to either provide a `text.query' input or a `query.field` + `query.field.value` input.") }
if (is.null(query.field) == FALSE && is.null(query.field.value) == TRUE) { stop("Please provide a value with a query field.") }
if (length(query.field) > 1) { stop('Please provide only one query field. Run rw.query.fields() if you are in doubt.') }
if (is.null(limit) == FALSE && limit < 0 && tolower(limit) != "all") { stop('Please provide an integer between 1 and 1000 or all.') }
if (is.null(limit) == FALSE && limit > 1000 && tolower(limit) != "all") { stop('Please provide an integer between 1 and 1000 or all.') }  # Increase the upper limit of the function.
if (is.null(limit) == FALSE) { limit <- tolower(limit) }
all <- "all"
#### Building the URL snippets and checking the validity of each parameter. ####
# Here we are building the query.url, param-by-param.
if (is.null(entity) == TRUE) { stop('Please provide an entity. \nAt this point only the `report` \nentity is fully implemented.') }
if (is.null(entity) == FALSE) { entity.url <- paste(entity, "/list", sep = "") }
if (is.null(limit) == TRUE) { limit.url <- paste("?limit=", 10, "&", sep = "")
warning("The default limit for this querier is 10. \nIf you need more please provide a number           using \nthe 'limit' parameter.") }
if (is.null(limit) == FALSE) { limit.url <- paste("?limit=",
ifelse(limit == "all", 1000, limit),
"&", sep = "") }
if (is.null(text.query) == TRUE) { text.query.url <- NULL }
if (is.null(text.query) == FALSE) {
text.query.url <- paste("query[value]=",
text.query,
sep = "")
warning('In this version searching the open text field \nwill override whatever other field you have\nincluded the `query` paramenter. In further \nversions the open text field will allow you to\nfurther refine your search.')
}
if (is.null(query.field) == FALSE) { query.field.url <- paste("query[value]=",
query.field,
":",
query.field.value,
sep = "") }
if (is.null(query.field) == TRUE) { query.field.url <- NULL }
# Function for building the right query when more than one field is provided.
many.fields <- function(qf = NULL) {
if (entity == "country") {
} else { ifelse(all(is.na(match(qf, 'date.created')) == TRUE), qf[length(qf) + 1] <- 'date.created', '') }
# date.created is a default field due to sorting -- unless country.
all.fields.url.list <- list()
for (i in 0:(length(qf) - 1)) {
field.url <- paste("fields[include][",i,"]=", qf[i + 1], sep = "")
all.fields.url.list[i + 1] <- paste("&", field.url, sep = "")
}
all.fields.url <- paste(all.fields.url.list, collapse = "")
return(all.fields.url)
}
if (is.null(add.fields) == FALSE) { add.fields.url <- many.fields(qf = add.fields) }
## From and to paramenters. ##
# if (is.null(from) == TRUE) {}  ## Implement in future versions.
# if (is.null(to) == TRUE) {}  ## Implement in future versions.
## Building URL for aquiring data. ##
api.url <- "http://api.rwlabs.org/v0/"
query.url <- paste(api.url,
entity.url,
limit.url,
text.query.url,
query.field.url,
add.fields.url,
ifelse(entity != "country", "&sort[0]=date.created:desc", ""),
sep = "")
#### Fetching the data. ####
if (debug == TRUE) {
x <- paste("The URL being queried is: ", query.url, sep = "")
warning(x)
}
# Getting the count number for iterations later.
count <- data.frame(fromJSON(getURLContent(query.url)))
#     count <- count$data.total[1]  # hacking querier temporarily for getting all the jobs.
count <- 61000  # latest figure from trends.rwlabs.org
#### Enhancement ####
# Here I am querying the url and getting a base data.frame.
# It would probably be better to implement this process within the iteration bellow.
# Using `jsonlite` to fetch JSON from the URL.
query <- data.frame(fromJSON(getURLContent(query.url)))
# Function to convert the resulting lits into rows in the data.frame
rw.fields <- function(df = NULL) {
for (i in 1:length(add.fields)) {
## Multi Fields ##
# Multi fields not implemented. Will be implemented in future version.
# if (length(df$data.list.fields[i]) > 1) { x <- paste(df$data.list.fields[i], collapse = ",") }
if (length(df$data.list.fields[i]) <= 1) { x <- data.frame(as.list(df$data.list.fields[i]))  }
df <- cbind(df, x)
}
df$data.list.fields <- NULL
return(df)
}
query <- rw.fields(df = query)
# Creating a metadata data.frame.
if (csv == TRUE) {
meta.data <- query[1, 1:7]
write.csv(meta.data, file = paste("data/",
paste(add.fields,
collapse = "-",
"-",
entity,
"-metadata.csv", sep = ""), row.names = FALSE))}
# UI element.
print(paste("Fetching ~", ifelse(is.null(limit) == TRUE, 10, ifelse(identical(limit,all) == TRUE, count, limit)),
" records.", sep = ""))
# Creating iterations to go around the 1000-results limitation.
rw.it <- function(df = NULL) {
if ('created' %in% names(df) == TRUE) { to <- df$created[nrow(df)] }
if ('date' %in% names(df) == TRUE) { to <- df$created[nrow(df)] }
final <- df
# Create progress bar.
limit <- ifelse(limit == "all", 1000, limit)
total <- ceiling(count/limit)
pb <- txtProgressBar(min = 0, max = total, style = 3)
for (i in 2:total) {
setTxtProgressBar(pb, i)  # Updates progress bar.
# Collects the latest date entry collected.
if ('created' %in% names(final) == TRUE) {
to <- format(final$created[nrow(final)], scientific = FALSE)
}
if ('date' %in% names(final) == TRUE) {
to <- format(final$created[nrow(final)], scientific = FALSE)
}
# Creates an URL with the latest date collected.
it.url <- paste(query.url, "&filter[field]=date.created&filter[value][to]=",
#                           ifelse(entity == "country",
#                                  "&filter[field]=date.created&filter[value][to]=", ""),
to, sep = "")
if (debug == TRUE) {
print(paste("This is the it.url", it.url, sep = ""))
print(paste("From iteration number ", i, sep = ""))
}
## Error handling function for each iteration.
tryCatch(x <- data.frame(fromJSON(getURLContent(it.url))),
error = function(e) {
print("There was an error in the URL queried. Skipping ...")
final <- final
},
finally = {
x <- rw.fields(df = x)  # Cleaning fields.
final <- rbind(final, x)
}
)
}
close(pb)
return(final)
}
# Only run iterator if we are fetching "all" entries.
# Note: improve to be more > 1000, but not 'all'.
if (identical(limit,all) == TRUE) { query <- rw.it(df = query) }
#### Cleaning the resulting data. ####
# Transform dates from Epoch to year-month-day.
rw.time <- function(df = NULL) {
df$created <- df$created/1000 # To eliminate the miliseconds.
df$created <- as.Date(as.POSIXct(as.numeric(df$created), origin = "1970-01-01"))
return(df)
}
query <- rw.time(df = query)
# Keeping only the columns of interest.
query <- query[,10:ncol(query)]
if (debug == TRUE) {
before.duplicates <- nrow(query)
}
query <- unique(query)
if (debug == TRUE) {
# UI element
after.duplicates <- nrow(query)
duplicates <- before.duplicates - after.duplicates
print(paste("There were ", duplicates, " duplicates in the query."))
}
#   # Storing the resulting data in a CSV file.
if (csv == TRUE) {
write.csv(query, file = "ReliefWeb-query", "-", entity, ".csv", sep = "")
}
print("Done.")
return(query)
}
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created', 'country.iso3'))
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created'))
View(all.jobs)
nrow(all.jobs)
all.jobs <- rw.query(entity = "job", limit = 'all', text.query = '', add.fields = c('id', 'title', 'url', 'date.created'), debug = TRUE)
nrow(all.jobs)
library(RCurl)
library(Rjson)
library(rjson)
date()
x <- date()
x
class(x)
system.time()
sys.time()
sys.time()
Sys.time()
x <- Sys.time()
class(x)
library(RCurl)
library(rjson)
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list"))
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list")
/getURL
?getURL
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
user = "dataproject", userpwd="humdata", httpauth = 1L))
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata", httpauth = 1L))
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata", httpauth = 1L)
getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="humdata")
?getURL
x <- fromJSON(getURL("http://test-data.hdx.rwlabs.org/api/3/action/user_list",
userpwd="dataproject:humdata", httpauth = 1L))
View(x)
x
names(x)
names(x)[2]
names(x)[2][1]
y <- names(x)[2]
names(y)
y
y <- names(x)[3]
names(y)
y
View(y)
y <- x[3]
y
length(y)
names(Y)
names(y)
y[1]
length(y[1])
nrow(y[1])
z <- y[1]
unclass(z)
nrow(unclass(z))
length(unclass(z))
nrow(data.frame(z))
data.frame(z)
data.frame(unclass(z))
z
classa(z)
class(z)
z
names(z)
names(z)[1]
names(z)[1][1]
z[1]
z[1][1]
z[[1]]
length(z[[1]])
z$names
z$name
z[1]$name
z[[1]$name
z[[1]]$name
z[[1]]
length(z[[1]])
z[[1]]$name
names(z[[1]])
data.frame(z[[1]])
XML
library(XML)
theurl <- "http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=21745&Itemid=5"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
tables <- readHTMLTable(theurl)
library(RCurl)
tables <- readHTMLTable(getURL(theurl))
tables <- readHTMLTable(getURL(theurl))
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows
tables
tables[1]
tables[[11]
tables[[1]]
class(tables)
View(tables)
tables <- readHTMLTable(getURL(theurl))
theurl <- (getURL(theurl)
theurl <- (getURL(theurl))
theurl <- (getURL(theurl))
tables <- readHTMLTable(theurl)
tables
getURL('http://www.ipea.gov.br/portal/index.php?option=com_content&view=article&id=21745&Itemid=5')
getURL('http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=13691&Itemid=5')
getURL('http://www.ipea.gov.br/portal/index.php/?option=com_content&view=article&id=13691')
library(rjson)
library(RCurl)
base_url <- 'https://ds-ec2.scraperwiki.com/ye7nhjd/uq816xboyfzffx6/sql'
table_name <- 'unhcr_real_time'  # table name with the viz data.
sql_query <- paste('?q=select%20*%20from%20', table_name, sep = "")
query_url <- getURL(paste(base_url, sql_query, sep = ""))
getData <- function() {
b <- fromJSON(query_url)
for (i in 1:length(b)) {
c <- b[[i]]
if (i == 1) d <- c
else d <- cbind(d, c)
}
z <- data.frame(t(d))
z
}
data <- getData()
# Leave only the columns of interest
# and store the resulting objects into
# different CSV files.
unhcr_real_time <- data.frame(as.character(data$name),
as.character(data$module_name),
as.character(data$module_type),
as.numeric(data$value),
as.character(data$updated_at))
View(unhcr_real_time)
unhcr_real_time$updated_at <- as.Date(unhcr_real_time$updated_at)
as.Date(unhcr_real_time$updated_at)
View(unhcr_real_time)
names(unhcr_real_time)
names(unhcr_real_time) <- c('Emergency', 'Module_Name', 'Modupe_Type', 'Value', 'Updated_at')
unhcr_real_time$Updated_at <- as.Date(unhcr_real_time$Updated_at)
View(Updated_at)
View(unhcr_real_time)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
temp
sum(x)
x
1 + 2 + 3 + 4 / 4
(1 + 2 + 3 + 4) / 4
(1 + 2 + 3 + 4) / 4
sum(x ^ 2 * p) - sum(x * p) ^ 2
x * y
x * p
y <- x * p
y / length(x)
sum(y) / length(x)
y
sum(y)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
g
movies
ggplot(movies, aes(votes, rating))
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + stat_smooth("loess")
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + geom_smooth(method = 'loess')
qplot(votes, rating, data = movies) + geom_smooth()  # works, but loess?
?geom_smooth
qplot(votes, rating, data = movies, panel = panel.loess)  # what?
library(lattice)
xyplot()
bwplot()
?bwplot()
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(RCurl)
library(rjson)
library(sqldf)
library(lubridate)
library(ggplot2)
# Storing the data in a SW database.
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
#     dbWriteTable(db, "data", data, row.names = FALSE, append = TRUE) # for append
dbListFields(db, "data")
NewData <- dbReadTable(db, "data")
dbDisconnect(db)
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
dbListTables(db)
dbDisconnect(db)
db <- dbConnect(SQLite(), dbname="scraperwiki.sqlite")
dbListTables(db)
?dbListTables
dbListTables(db)
NewData <- dbReadTable(db, "data")
NewData <- dbReadTable(db, "value")
library(XML)
library(RCurl)
library(countrycode)
url
base_url <- 'http://www.unocha.org/cap/appeals/by-appeal/results'
page <- '?page='  # 492 appeals in 11 pages -- starts at 0.
url <- paste0(base_url, page, 2)
url
doc <- htmlParse(url)
data.frame(
hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
x <- data.frame(hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
View(x)
x <- data.frame(
names = xpathApply(doc, '//*[@class = "views-field views-field-title"', xmlValue)
hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
x <- data.frame(
names = xpathApply(doc, '//*[@class = "views-field views-field-title"', xmlValue),
hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
x <- data.frame(
names = xpathApply(doc, '//*[@class = "views-field views-field-title"]', xmlValue),
hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
View(x)
ncol(x)
nrow(x)
x <- data.frame(
names = xpathApply(doc, '//*[@class = "views-field views-field-title"]', xmlValue)
#     hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href')
)
View(x)
names
t(x)
y <- t(x)
View(Y)
View(y)
x <- data.frame(
names = xpathApply(doc, '//*[@class="views-field views-field-title"]/text()', xmlValue)
#     hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href')
)
View(x)
x <- data.frame(
names = xpathApply(doc, '//*[@class="views-field views-field-title"]', xmlValue)
#     hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href')
)
View(x)
y <- t(x)
View(y)
?xpathSApply
#### Extracting Links  ####
getLinks <- function() <- {
pb <- txtProgressBar(min = 0, max = 9, style = 3)
for(i in 0:9) {
setProgressBar(pb, i)
url <- paste0(base_url, page, i)
doc <- htmlParse(url)
link_it <- data.frame(hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
if (i == 0) link_list <- link_it
else link_list <- rbind(link_list, link_it)
}
return(link_list)
}
getLinks <- function() <- {
pb <- txtProgressBar(min = 0, max = 9, style = 3)
for(i in 0:9) {
setTxtProgressBar(pb, i)
url <- paste0(base_url, page, i)
doc <- htmlParse(url)
link_it <- data.frame(hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
if (i == 0) link_list <- link_it
else link_list <- rbind(link_list, link_it)
}
getLinks <- function() {
pb <- txtProgressBar(min = 0, max = 9, style = 3)
for(i in 0:9) {
setTxtProgressBar(pb, i)
url <- paste0(base_url, page, i)
doc <- htmlParse(url)
link_it <- data.frame(hrefs =  xpathSApply(doc, '//*[@class="views-field views-field-field-dms-link-value"]/a',xmlGetAttr, 'href'))
if (i == 0) link_list <- link_it
else link_list <- rbind(link_list, link_it)
}
return(link_list)
}
link_list <- getLinks()
View(link_list)
write.csv(link_list, 'data/link_list.csv', row.names = F)
setwd("~/Documents/Programming/cap_appeals")
write.csv(link_list, 'data/link_list.csv', row.names = F)
?download.file
link_list$file_name <- gsub('(.+?)/', "", link_list$href)
View(link_list)
for(i in 1:nrow(link_list)) {
download.file(link_list$href[i], destfile = link_list$file_name[i])
}
for(i in 1:nrow(link_list)) {
download.file(as.character(link_list$href[i]), destfile = link_list$file_name[i])
}
for(i in 1:nrow(link_list)) {
url <- as.character(link_list$href[i])
file_name <- link_list$file_name[i]
download.file(url, file_name, method = 'curl')
}
